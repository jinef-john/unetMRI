Starting Fixed Adversarial Watermarking Training...
Loading models...
Loading C1 model...
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(C1_PATH, map_location=DEVICE)
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:265: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(AE_PATH, map_location=DEVICE)
Starting training...
\n=== EPOCH 1/5 ===
WARMUP STAGE - Training C2 normally
ðŸ”„ Creating dataloader for warmup stage...
ðŸ“¦ Creating dataloader: warmup stage, batch_size=64
Batch 0: C2_loss=2.820, C1_acc=1.000, C2_acc=0.188
Batch 10: C2_loss=1.232, C1_acc=1.000, C2_acc=0.484
Batch 20: C2_loss=0.999, C1_acc=0.984, C2_acc=0.641
Batch 30: C2_loss=1.002, C1_acc=1.000, C2_acc=0.547
Batch 40: C2_loss=0.967, C1_acc=1.000, C2_acc=0.594
Batch 50: C2_loss=0.799, C1_acc=1.000, C2_acc=0.688
Batch 60: C2_loss=1.024, C1_acc=1.000, C2_acc=0.750
Batch 70: C2_loss=0.676, C1_acc=1.000, C2_acc=0.719
Batch 80: C2_loss=0.720, C1_acc=1.000, C2_acc=0.750
Epoch 1 Summary:
  Stage: warmup
  C1 Performance: clean=0.998
  C2 Performance: clean=0.571
\n=== EPOCH 2/5 ===
ADVERSARIAL STAGE - C2 vs Generator
ðŸ”„ Creating dataloader for adversarial stage...
ðŸ“¦ Creating dataloader: adversarial stage, batch_size=32
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 812, in <module>
    main()
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 701, in main
    metrics = trainer.train_step_adversarial(batch, intensity, mask_fraction)
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 518, in train_step_adversarial
    wm_latent, wm_skip = self.generate_watermark(img, labels, intensity)
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 411, in generate_watermark
    wm_latent, wm_skip = self.watermark_gen(img, intensity)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 204, in forward
    wm_skip_full = self.out_skip(d2)  # 512x512
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.20 GiB of which 5.30 GiB is free. Process 97545 has 73.89 GiB memory in use. Of the allocated memory 70.13 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Starting Fixed Adversarial Watermarking Training...
Loading models...
Loading C1 model...
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(C1_PATH, map_location=DEVICE)
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:265: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(AE_PATH, map_location=DEVICE)
Starting training...
\n=== EPOCH 1/5 ===
WARMUP STAGE - Training C2 normally
ðŸ”„ Creating dataloader for warmup stage...
ðŸ“¦ Creating dataloader: warmup stage, batch_size=64
Batch 0: C2_loss=2.705, C1_acc=1.000, C2_acc=0.234
Batch 10: C2_loss=2.007, C1_acc=0.984, C2_acc=0.391
Batch 20: C2_loss=1.351, C1_acc=1.000, C2_acc=0.438
Batch 30: C2_loss=1.278, C1_acc=1.000, C2_acc=0.500
Batch 40: C2_loss=1.291, C1_acc=1.000, C2_acc=0.500
Batch 50: C2_loss=1.228, C1_acc=1.000, C2_acc=0.547
Batch 60: C2_loss=0.823, C1_acc=1.000, C2_acc=0.656
Batch 70: C2_loss=1.037, C1_acc=1.000, C2_acc=0.609
Batch 80: C2_loss=0.846, C1_acc=1.000, C2_acc=0.609
Epoch 1 Summary:
  Stage: warmup
  C1 Performance: clean=0.998
  C2 Performance: clean=0.519
\n=== EPOCH 2/5 ===
ADVERSARIAL STAGE - C2 vs Generator
ðŸ”„ Creating dataloader for adversarial stage...
ðŸ“¦ Creating dataloader: adversarial stage, batch_size=8
Batch 0: C2_loss=7.015, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.375
  Quality: L1=0.0003
Batch 10: C2_loss=4.465, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.625
  Quality: L1=0.0002
Batch 20: C2_loss=8.836, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.625
  Quality: L1=0.0001
Batch 30: C2_loss=11.923, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.125
  Quality: L1=0.0001
Batch 40: C2_loss=15.292, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.125
  Quality: L1=0.0002
Batch 50: C2_loss=4.809, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.750
  Quality: L1=0.0002
Batch 60: C2_loss=4.905, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.750, wm=0.625
  Quality: L1=0.0001
Batch 70: C2_loss=3.625, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.750
  Quality: L1=0.0002
Batch 80: C2_loss=7.502, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.625
  Quality: L1=0.0002
Batch 90: C2_loss=6.546, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.375
  Quality: L1=0.0002
Batch 100: C2_loss=7.132, Gen_loss=226213.688
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.375
  Quality: L1=0.0001
Batch 110: C2_loss=4.403, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.750, wm=0.875
  Quality: L1=0.0002
Batch 120: C2_loss=3.502, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.875
  Quality: L1=0.0002
Batch 130: C2_loss=3.376, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.750, wm=0.750
  Quality: L1=0.0002
Batch 140: C2_loss=6.085, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.500
  Quality: L1=0.0002
Batch 150: C2_loss=8.495, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.250
  Quality: L1=0.0001
Batch 160: C2_loss=8.376, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.750, wm=0.625
  Quality: L1=0.0002
Batch 170: C2_loss=10.061, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.375
  Quality: L1=0.0001
Batch 180: C2_loss=6.851, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.500
  Quality: L1=0.0001
Batch 190: C2_loss=4.024, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.625
  Quality: L1=0.0001
Batch 200: C2_loss=9.902, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.000, wm=0.375
  Quality: L1=0.0001
Batch 210: C2_loss=3.512, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.875, wm=0.750
  Quality: L1=0.0001
Batch 220: C2_loss=4.967, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.375
  Quality: L1=0.0001
Batch 230: C2_loss=3.848, Gen_loss=471472.188
  C1: clean=1.000, wm=1.000
  C2: clean=0.750, wm=0.625
  Quality: L1=0.0001
Batch 240: C2_loss=8.828, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.250
  Quality: L1=0.0001
Batch 250: C2_loss=8.490, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.375
  Quality: L1=0.0001
Batch 260: C2_loss=4.969, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.625
  Quality: L1=0.0001
Batch 270: C2_loss=3.430, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.750, wm=0.625
  Quality: L1=0.0001
Batch 280: C2_loss=2.510, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.750, wm=0.875
  Quality: L1=0.0001
Batch 290: C2_loss=4.024, Gen_loss=0.000
  C1: clean=1.000, wm=1.000
  C2: clean=0.750, wm=0.625
  Quality: L1=0.0001
