Starting Fixed Adversarial Watermarking Training...
Loading models...
Loading C1 model...
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(C1_PATH, map_location=DEVICE)
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:258: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(AE_PATH, map_location=DEVICE)
Starting training...
\n=== EPOCH 1/50 ===
WARMUP STAGE - Training C2 normally
Batch 0: C2_loss=2.283, C1_acc=1.000, C2_acc=0.250
Batch 10: C2_loss=6.436, C1_acc=1.000, C2_acc=0.375
Batch 20: C2_loss=5.589, C1_acc=1.000, C2_acc=0.500
Batch 30: C2_loss=2.934, C1_acc=1.000, C2_acc=0.125
Batch 40: C2_loss=1.264, C1_acc=1.000, C2_acc=0.500
Batch 50: C2_loss=1.792, C1_acc=1.000, C2_acc=0.375
Batch 60: C2_loss=1.026, C1_acc=1.000, C2_acc=0.500
Batch 70: C2_loss=1.837, C1_acc=1.000, C2_acc=0.500
Batch 80: C2_loss=3.819, C1_acc=1.000, C2_acc=0.000
Batch 90: C2_loss=2.892, C1_acc=1.000, C2_acc=0.375
Batch 100: C2_loss=1.663, C1_acc=1.000, C2_acc=0.375
Batch 110: C2_loss=1.425, C1_acc=1.000, C2_acc=0.250
Batch 120: C2_loss=0.905, C1_acc=1.000, C2_acc=0.750
Batch 130: C2_loss=0.958, C1_acc=1.000, C2_acc=0.750
Batch 140: C2_loss=1.049, C1_acc=1.000, C2_acc=0.375
Batch 150: C2_loss=1.181, C1_acc=1.000, C2_acc=0.375
Batch 160: C2_loss=1.038, C1_acc=1.000, C2_acc=0.375
Batch 170: C2_loss=1.207, C1_acc=1.000, C2_acc=0.500
Batch 180: C2_loss=1.019, C1_acc=1.000, C2_acc=0.625
Batch 190: C2_loss=1.560, C1_acc=1.000, C2_acc=0.250
Batch 200: C2_loss=1.006, C1_acc=1.000, C2_acc=0.375
Batch 210: C2_loss=1.100, C1_acc=1.000, C2_acc=0.375
Batch 220: C2_loss=0.848, C1_acc=1.000, C2_acc=0.625
Batch 230: C2_loss=1.770, C1_acc=1.000, C2_acc=0.375
Batch 240: C2_loss=0.954, C1_acc=1.000, C2_acc=0.625
Batch 250: C2_loss=0.839, C1_acc=1.000, C2_acc=0.750
Batch 260: C2_loss=1.245, C1_acc=1.000, C2_acc=0.375
Batch 270: C2_loss=1.395, C1_acc=1.000, C2_acc=0.500
Batch 280: C2_loss=0.710, C1_acc=1.000, C2_acc=0.875
Batch 290: C2_loss=0.732, C1_acc=1.000, C2_acc=0.625
Batch 300: C2_loss=0.555, C1_acc=1.000, C2_acc=0.875
Batch 310: C2_loss=1.032, C1_acc=1.000, C2_acc=0.500
Batch 320: C2_loss=1.066, C1_acc=1.000, C2_acc=0.625
Batch 330: C2_loss=0.796, C1_acc=1.000, C2_acc=0.625
Batch 340: C2_loss=1.352, C1_acc=1.000, C2_acc=0.375
Batch 350: C2_loss=0.829, C1_acc=1.000, C2_acc=0.500
Batch 360: C2_loss=1.369, C1_acc=1.000, C2_acc=0.250
Batch 370: C2_loss=1.531, C1_acc=1.000, C2_acc=0.250
Batch 380: C2_loss=0.997, C1_acc=1.000, C2_acc=0.500
Batch 390: C2_loss=1.178, C1_acc=1.000, C2_acc=0.375
Batch 400: C2_loss=1.180, C1_acc=1.000, C2_acc=0.250
Batch 410: C2_loss=1.080, C1_acc=1.000, C2_acc=0.500
Batch 420: C2_loss=0.578, C1_acc=1.000, C2_acc=0.875
Batch 430: C2_loss=0.667, C1_acc=1.000, C2_acc=0.625
Batch 440: C2_loss=0.729, C1_acc=1.000, C2_acc=0.625
Batch 450: C2_loss=0.625, C1_acc=1.000, C2_acc=0.750
Batch 460: C2_loss=2.005, C1_acc=1.000, C2_acc=0.375
Batch 470: C2_loss=1.047, C1_acc=1.000, C2_acc=0.750
Batch 480: C2_loss=0.520, C1_acc=1.000, C2_acc=1.000
Batch 490: C2_loss=1.117, C1_acc=1.000, C2_acc=0.500
Batch 500: C2_loss=0.981, C1_acc=1.000, C2_acc=0.500
Batch 510: C2_loss=1.608, C1_acc=1.000, C2_acc=0.375
Batch 520: C2_loss=0.601, C1_acc=1.000, C2_acc=0.875
Batch 530: C2_loss=0.994, C1_acc=1.000, C2_acc=0.375
Batch 540: C2_loss=0.727, C1_acc=1.000, C2_acc=0.500
Batch 550: C2_loss=0.578, C1_acc=1.000, C2_acc=0.750
Batch 560: C2_loss=1.148, C1_acc=1.000, C2_acc=0.500
Batch 570: C2_loss=1.149, C1_acc=1.000, C2_acc=0.500
Batch 580: C2_loss=0.862, C1_acc=1.000, C2_acc=0.500
Batch 590: C2_loss=0.765, C1_acc=1.000, C2_acc=0.750
Batch 600: C2_loss=1.116, C1_acc=1.000, C2_acc=0.625
Batch 610: C2_loss=1.223, C1_acc=1.000, C2_acc=0.250
Batch 620: C2_loss=0.933, C1_acc=1.000, C2_acc=0.500
Batch 630: C2_loss=0.615, C1_acc=1.000, C2_acc=0.625
Batch 640: C2_loss=0.435, C1_acc=1.000, C2_acc=0.750
Batch 650: C2_loss=1.031, C1_acc=1.000, C2_acc=0.375
Batch 660: C2_loss=0.822, C1_acc=1.000, C2_acc=0.500
Batch 670: C2_loss=0.487, C1_acc=1.000, C2_acc=0.875
Batch 680: C2_loss=0.504, C1_acc=1.000, C2_acc=0.875
Batch 690: C2_loss=0.759, C1_acc=1.000, C2_acc=0.750
Batch 700: C2_loss=1.435, C1_acc=1.000, C2_acc=0.625
Batch 710: C2_loss=0.757, C1_acc=1.000, C2_acc=0.875
Epoch 1 Summary:
  Stage: warmup
\n=== EPOCH 2/50 ===
WARMUP STAGE - Training C2 normally
Batch 0: C2_loss=0.361, C1_acc=1.000, C2_acc=0.750
Batch 10: C2_loss=1.485, C1_acc=1.000, C2_acc=0.375
Batch 20: C2_loss=0.975, C1_acc=1.000, C2_acc=0.500
Batch 30: C2_loss=1.702, C1_acc=1.000, C2_acc=0.250
Batch 40: C2_loss=1.341, C1_acc=1.000, C2_acc=0.250
Batch 50: C2_loss=0.877, C1_acc=1.000, C2_acc=0.500
Batch 60: C2_loss=0.885, C1_acc=1.000, C2_acc=0.750
Batch 70: C2_loss=1.007, C1_acc=1.000, C2_acc=0.375
Batch 80: C2_loss=0.967, C1_acc=1.000, C2_acc=0.750
Batch 90: C2_loss=0.995, C1_acc=1.000, C2_acc=0.500
Batch 100: C2_loss=1.091, C1_acc=1.000, C2_acc=0.625
Batch 110: C2_loss=1.068, C1_acc=1.000, C2_acc=0.375
Batch 120: C2_loss=0.988, C1_acc=1.000, C2_acc=0.625
Batch 130: C2_loss=0.900, C1_acc=1.000, C2_acc=0.750
Batch 140: C2_loss=2.062, C1_acc=1.000, C2_acc=0.000
Batch 150: C2_loss=0.820, C1_acc=1.000, C2_acc=0.625
Batch 160: C2_loss=0.906, C1_acc=1.000, C2_acc=0.750
Batch 170: C2_loss=1.190, C1_acc=1.000, C2_acc=0.625
Batch 180: C2_loss=0.754, C1_acc=1.000, C2_acc=0.625
Batch 190: C2_loss=0.571, C1_acc=1.000, C2_acc=0.875
Batch 200: C2_loss=0.913, C1_acc=1.000, C2_acc=0.625
Batch 210: C2_loss=1.041, C1_acc=1.000, C2_acc=0.625
Batch 220: C2_loss=0.998, C1_acc=1.000, C2_acc=0.375
Batch 230: C2_loss=1.078, C1_acc=1.000, C2_acc=0.500
Batch 240: C2_loss=1.152, C1_acc=1.000, C2_acc=0.625
Batch 250: C2_loss=0.896, C1_acc=1.000, C2_acc=0.625
Batch 260: C2_loss=1.740, C1_acc=1.000, C2_acc=0.375
Batch 270: C2_loss=0.904, C1_acc=1.000, C2_acc=0.500
Batch 280: C2_loss=0.447, C1_acc=1.000, C2_acc=0.875
Batch 290: C2_loss=0.843, C1_acc=1.000, C2_acc=0.750
Batch 300: C2_loss=0.554, C1_acc=1.000, C2_acc=0.750
Batch 310: C2_loss=1.295, C1_acc=1.000, C2_acc=0.625
Batch 320: C2_loss=0.839, C1_acc=1.000, C2_acc=0.500
Batch 330: C2_loss=0.710, C1_acc=1.000, C2_acc=0.750
Batch 340: C2_loss=0.729, C1_acc=1.000, C2_acc=0.750
Batch 350: C2_loss=0.262, C1_acc=1.000, C2_acc=1.000
Batch 360: C2_loss=1.055, C1_acc=1.000, C2_acc=0.625
Batch 370: C2_loss=0.660, C1_acc=1.000, C2_acc=0.875
Batch 380: C2_loss=0.823, C1_acc=1.000, C2_acc=0.875
Batch 390: C2_loss=2.636, C1_acc=1.000, C2_acc=0.250
Batch 400: C2_loss=0.897, C1_acc=0.875, C2_acc=0.500
Batch 410: C2_loss=0.701, C1_acc=1.000, C2_acc=0.750
Batch 420: C2_loss=1.657, C1_acc=1.000, C2_acc=0.375
Batch 430: C2_loss=0.967, C1_acc=1.000, C2_acc=0.625
Batch 440: C2_loss=1.213, C1_acc=1.000, C2_acc=0.500
Batch 450: C2_loss=0.619, C1_acc=1.000, C2_acc=0.875
Batch 460: C2_loss=0.329, C1_acc=1.000, C2_acc=0.875
Batch 470: C2_loss=0.865, C1_acc=1.000, C2_acc=0.500
Batch 480: C2_loss=0.786, C1_acc=1.000, C2_acc=0.750
Batch 490: C2_loss=1.129, C1_acc=1.000, C2_acc=0.500
Batch 500: C2_loss=0.901, C1_acc=1.000, C2_acc=0.625
Batch 510: C2_loss=0.569, C1_acc=1.000, C2_acc=0.625
Batch 520: C2_loss=0.467, C1_acc=1.000, C2_acc=0.625
Batch 530: C2_loss=0.682, C1_acc=1.000, C2_acc=0.500
Batch 540: C2_loss=0.545, C1_acc=1.000, C2_acc=0.875
Batch 550: C2_loss=0.670, C1_acc=1.000, C2_acc=0.750
Batch 560: C2_loss=0.951, C1_acc=1.000, C2_acc=0.500
Batch 570: C2_loss=0.981, C1_acc=1.000, C2_acc=0.375
Batch 580: C2_loss=0.948, C1_acc=1.000, C2_acc=0.375
Batch 590: C2_loss=1.437, C1_acc=1.000, C2_acc=0.625
Batch 600: C2_loss=1.490, C1_acc=1.000, C2_acc=0.250
Batch 610: C2_loss=0.501, C1_acc=1.000, C2_acc=0.750
Batch 620: C2_loss=0.618, C1_acc=1.000, C2_acc=0.875
Batch 630: C2_loss=1.069, C1_acc=1.000, C2_acc=0.500
Batch 640: C2_loss=0.732, C1_acc=1.000, C2_acc=0.750
Batch 650: C2_loss=0.434, C1_acc=1.000, C2_acc=0.875
Batch 660: C2_loss=0.755, C1_acc=1.000, C2_acc=0.750
Batch 670: C2_loss=0.328, C1_acc=1.000, C2_acc=1.000
Batch 680: C2_loss=0.173, C1_acc=1.000, C2_acc=1.000
Batch 690: C2_loss=0.764, C1_acc=1.000, C2_acc=0.375
Batch 700: C2_loss=0.539, C1_acc=1.000, C2_acc=0.750
Batch 710: C2_loss=0.341, C1_acc=1.000, C2_acc=0.875
Epoch 2 Summary:
  Stage: warmup
\n=== EPOCH 3/50 ===
WARMUP STAGE - Training C2 normally
Batch 0: C2_loss=1.148, C1_acc=1.000, C2_acc=0.500
Batch 10: C2_loss=0.469, C1_acc=1.000, C2_acc=0.875
Batch 20: C2_loss=0.662, C1_acc=1.000, C2_acc=0.875
Batch 30: C2_loss=0.505, C1_acc=1.000, C2_acc=0.750
Batch 40: C2_loss=0.845, C1_acc=1.000, C2_acc=0.625
Batch 50: C2_loss=0.770, C1_acc=1.000, C2_acc=0.625
Batch 60: C2_loss=0.547, C1_acc=1.000, C2_acc=0.875
Batch 70: C2_loss=1.948, C1_acc=1.000, C2_acc=0.500
Batch 80: C2_loss=0.807, C1_acc=1.000, C2_acc=0.625
Batch 90: C2_loss=1.180, C1_acc=1.000, C2_acc=0.625
Batch 100: C2_loss=0.779, C1_acc=1.000, C2_acc=0.500
Batch 110: C2_loss=0.544, C1_acc=1.000, C2_acc=0.875
Batch 120: C2_loss=0.701, C1_acc=1.000, C2_acc=0.875
Batch 130: C2_loss=0.692, C1_acc=1.000, C2_acc=0.750
Batch 140: C2_loss=0.769, C1_acc=1.000, C2_acc=0.500
Batch 150: C2_loss=1.016, C1_acc=1.000, C2_acc=0.875
Batch 160: C2_loss=0.729, C1_acc=1.000, C2_acc=0.750
Batch 170: C2_loss=0.661, C1_acc=0.875, C2_acc=0.750
Batch 180: C2_loss=0.851, C1_acc=1.000, C2_acc=0.625
Batch 190: C2_loss=1.423, C1_acc=1.000, C2_acc=0.500
Batch 200: C2_loss=0.760, C1_acc=1.000, C2_acc=0.750
Batch 210: C2_loss=0.872, C1_acc=1.000, C2_acc=0.500
Batch 220: C2_loss=0.380, C1_acc=1.000, C2_acc=0.875
Batch 230: C2_loss=0.346, C1_acc=1.000, C2_acc=0.875
Batch 240: C2_loss=0.763, C1_acc=1.000, C2_acc=0.500
Batch 250: C2_loss=0.450, C1_acc=1.000, C2_acc=0.875
Batch 260: C2_loss=0.585, C1_acc=1.000, C2_acc=0.875
Batch 270: C2_loss=1.365, C1_acc=1.000, C2_acc=0.625
Batch 280: C2_loss=0.575, C1_acc=1.000, C2_acc=0.750
Batch 290: C2_loss=0.932, C1_acc=1.000, C2_acc=0.750
Batch 300: C2_loss=0.964, C1_acc=1.000, C2_acc=0.625
Batch 310: C2_loss=0.978, C1_acc=1.000, C2_acc=0.500
Batch 320: C2_loss=0.804, C1_acc=1.000, C2_acc=0.750
Batch 330: C2_loss=0.445, C1_acc=1.000, C2_acc=1.000
Batch 340: C2_loss=1.183, C1_acc=1.000, C2_acc=0.375
Batch 350: C2_loss=0.760, C1_acc=1.000, C2_acc=0.750
Batch 360: C2_loss=0.406, C1_acc=1.000, C2_acc=0.875
Batch 370: C2_loss=1.112, C1_acc=1.000, C2_acc=0.500
Batch 380: C2_loss=0.527, C1_acc=1.000, C2_acc=0.625
Batch 390: C2_loss=0.500, C1_acc=1.000, C2_acc=0.875
Batch 400: C2_loss=0.627, C1_acc=1.000, C2_acc=0.750
Batch 410: C2_loss=1.187, C1_acc=1.000, C2_acc=0.500
Batch 420: C2_loss=0.525, C1_acc=1.000, C2_acc=0.750
Batch 430: C2_loss=0.789, C1_acc=1.000, C2_acc=0.625
Batch 440: C2_loss=0.988, C1_acc=1.000, C2_acc=0.625
Batch 450: C2_loss=1.130, C1_acc=1.000, C2_acc=0.625
Batch 460: C2_loss=1.179, C1_acc=1.000, C2_acc=0.500
Batch 470: C2_loss=0.935, C1_acc=1.000, C2_acc=0.375
Batch 480: C2_loss=0.693, C1_acc=1.000, C2_acc=0.625
Batch 490: C2_loss=1.496, C1_acc=1.000, C2_acc=0.375
Batch 500: C2_loss=0.452, C1_acc=1.000, C2_acc=0.875
Batch 510: C2_loss=0.394, C1_acc=1.000, C2_acc=0.875
Batch 520: C2_loss=1.369, C1_acc=1.000, C2_acc=0.750
Batch 530: C2_loss=0.421, C1_acc=1.000, C2_acc=0.875
Batch 540: C2_loss=0.424, C1_acc=1.000, C2_acc=0.875
Batch 550: C2_loss=0.418, C1_acc=1.000, C2_acc=0.750
Batch 560: C2_loss=0.705, C1_acc=1.000, C2_acc=0.750
Batch 570: C2_loss=0.096, C1_acc=1.000, C2_acc=1.000
Batch 580: C2_loss=0.944, C1_acc=1.000, C2_acc=0.625
Batch 590: C2_loss=0.682, C1_acc=1.000, C2_acc=0.625
Batch 600: C2_loss=0.661, C1_acc=1.000, C2_acc=0.625
Batch 610: C2_loss=0.314, C1_acc=1.000, C2_acc=1.000
Batch 620: C2_loss=0.347, C1_acc=1.000, C2_acc=1.000
Batch 630: C2_loss=0.485, C1_acc=1.000, C2_acc=0.750
Batch 640: C2_loss=0.698, C1_acc=1.000, C2_acc=0.625
Batch 650: C2_loss=0.288, C1_acc=1.000, C2_acc=0.875
Batch 660: C2_loss=0.642, C1_acc=1.000, C2_acc=0.750
Batch 670: C2_loss=0.433, C1_acc=1.000, C2_acc=0.750
Batch 680: C2_loss=0.493, C1_acc=1.000, C2_acc=0.750
Batch 690: C2_loss=1.075, C1_acc=1.000, C2_acc=0.500
Batch 700: C2_loss=0.661, C1_acc=1.000, C2_acc=0.625
Batch 710: C2_loss=0.510, C1_acc=0.875, C2_acc=0.750
Epoch 3 Summary:
  Stage: warmup
\n=== EPOCH 4/50 ===
WARMUP STAGE - Training C2 normally
Batch 0: C2_loss=0.845, C1_acc=1.000, C2_acc=0.750
Batch 10: C2_loss=0.678, C1_acc=1.000, C2_acc=0.750
Batch 20: C2_loss=0.171, C1_acc=1.000, C2_acc=1.000
Batch 30: C2_loss=0.406, C1_acc=1.000, C2_acc=0.750
Batch 40: C2_loss=0.626, C1_acc=1.000, C2_acc=0.750
Batch 50: C2_loss=0.761, C1_acc=1.000, C2_acc=0.750
Batch 60: C2_loss=0.540, C1_acc=1.000, C2_acc=0.750
Batch 70: C2_loss=0.473, C1_acc=0.875, C2_acc=0.875
Batch 80: C2_loss=0.824, C1_acc=1.000, C2_acc=0.750
Batch 90: C2_loss=1.376, C1_acc=1.000, C2_acc=0.625
Batch 100: C2_loss=0.794, C1_acc=1.000, C2_acc=0.750
Batch 110: C2_loss=0.974, C1_acc=1.000, C2_acc=0.750
Batch 120: C2_loss=0.611, C1_acc=1.000, C2_acc=0.875
Batch 130: C2_loss=0.346, C1_acc=1.000, C2_acc=0.875
Batch 140: C2_loss=0.472, C1_acc=1.000, C2_acc=0.875
Batch 150: C2_loss=1.142, C1_acc=1.000, C2_acc=0.625
Batch 160: C2_loss=0.972, C1_acc=1.000, C2_acc=0.625
Batch 170: C2_loss=0.561, C1_acc=1.000, C2_acc=0.625
Batch 180: C2_loss=0.572, C1_acc=1.000, C2_acc=0.750
Batch 190: C2_loss=0.783, C1_acc=1.000, C2_acc=0.625
Batch 200: C2_loss=0.568, C1_acc=1.000, C2_acc=0.750
Batch 210: C2_loss=0.666, C1_acc=1.000, C2_acc=0.750
Batch 220: C2_loss=0.775, C1_acc=1.000, C2_acc=0.750
Batch 230: C2_loss=0.406, C1_acc=1.000, C2_acc=0.750
Batch 240: C2_loss=0.659, C1_acc=1.000, C2_acc=0.500
Batch 250: C2_loss=0.746, C1_acc=1.000, C2_acc=0.625
Batch 260: C2_loss=0.566, C1_acc=1.000, C2_acc=0.750
Batch 270: C2_loss=0.388, C1_acc=1.000, C2_acc=0.875
Batch 280: C2_loss=0.409, C1_acc=1.000, C2_acc=0.875
Batch 290: C2_loss=0.683, C1_acc=1.000, C2_acc=0.625
Batch 300: C2_loss=0.752, C1_acc=1.000, C2_acc=0.750
Batch 310: C2_loss=0.461, C1_acc=1.000, C2_acc=0.875
Batch 320: C2_loss=0.460, C1_acc=1.000, C2_acc=0.875
Batch 330: C2_loss=0.642, C1_acc=1.000, C2_acc=0.750
Batch 340: C2_loss=0.406, C1_acc=1.000, C2_acc=0.875
Batch 350: C2_loss=0.275, C1_acc=1.000, C2_acc=1.000
Batch 360: C2_loss=0.747, C1_acc=1.000, C2_acc=0.625
Batch 370: C2_loss=0.751, C1_acc=1.000, C2_acc=0.750
Batch 380: C2_loss=0.914, C1_acc=1.000, C2_acc=0.500
Batch 390: C2_loss=0.242, C1_acc=1.000, C2_acc=1.000
Batch 400: C2_loss=0.238, C1_acc=1.000, C2_acc=1.000
Batch 410: C2_loss=1.337, C1_acc=1.000, C2_acc=0.250
Batch 420: C2_loss=0.598, C1_acc=1.000, C2_acc=0.625
Batch 430: C2_loss=0.569, C1_acc=1.000, C2_acc=0.750
Batch 440: C2_loss=0.496, C1_acc=1.000, C2_acc=0.875
Batch 450: C2_loss=0.573, C1_acc=1.000, C2_acc=0.750
Batch 460: C2_loss=0.714, C1_acc=1.000, C2_acc=0.625
Batch 470: C2_loss=0.358, C1_acc=1.000, C2_acc=1.000
Batch 480: C2_loss=0.615, C1_acc=1.000, C2_acc=0.625
Batch 490: C2_loss=0.441, C1_acc=1.000, C2_acc=0.875
Batch 500: C2_loss=1.466, C1_acc=1.000, C2_acc=0.500
Batch 510: C2_loss=0.548, C1_acc=1.000, C2_acc=0.750
Batch 520: C2_loss=0.575, C1_acc=1.000, C2_acc=0.625
Batch 530: C2_loss=0.390, C1_acc=1.000, C2_acc=0.875
Batch 540: C2_loss=0.878, C1_acc=1.000, C2_acc=0.750
Batch 550: C2_loss=1.267, C1_acc=1.000, C2_acc=0.625
Batch 560: C2_loss=0.331, C1_acc=1.000, C2_acc=0.875
Batch 570: C2_loss=1.166, C1_acc=1.000, C2_acc=0.500
Batch 580: C2_loss=0.530, C1_acc=1.000, C2_acc=0.750
Batch 590: C2_loss=0.663, C1_acc=1.000, C2_acc=0.750
Batch 600: C2_loss=0.221, C1_acc=1.000, C2_acc=0.875
Batch 610: C2_loss=0.244, C1_acc=1.000, C2_acc=0.875
Batch 620: C2_loss=0.386, C1_acc=1.000, C2_acc=0.875
Batch 630: C2_loss=0.627, C1_acc=1.000, C2_acc=0.750
Batch 640: C2_loss=0.622, C1_acc=1.000, C2_acc=0.750
Batch 650: C2_loss=0.865, C1_acc=1.000, C2_acc=0.750
Batch 660: C2_loss=0.925, C1_acc=1.000, C2_acc=0.750
Batch 670: C2_loss=0.567, C1_acc=1.000, C2_acc=0.875
Batch 680: C2_loss=1.085, C1_acc=1.000, C2_acc=0.375
Batch 690: C2_loss=0.665, C1_acc=1.000, C2_acc=0.750
Batch 700: C2_loss=0.354, C1_acc=1.000, C2_acc=0.875
Batch 710: C2_loss=0.314, C1_acc=1.000, C2_acc=0.875
Epoch 4 Summary:
  Stage: warmup
\n=== EPOCH 5/50 ===
WARMUP STAGE - Training C2 normally
Batch 0: C2_loss=0.657, C1_acc=1.000, C2_acc=0.750
Batch 10: C2_loss=0.170, C1_acc=1.000, C2_acc=0.875
Batch 20: C2_loss=0.406, C1_acc=1.000, C2_acc=0.875
Batch 30: C2_loss=0.425, C1_acc=1.000, C2_acc=0.875
Batch 40: C2_loss=0.842, C1_acc=1.000, C2_acc=0.500
Batch 50: C2_loss=0.788, C1_acc=1.000, C2_acc=0.750
Batch 60: C2_loss=0.493, C1_acc=1.000, C2_acc=0.750
Batch 70: C2_loss=0.324, C1_acc=1.000, C2_acc=0.875
Batch 80: C2_loss=1.168, C1_acc=1.000, C2_acc=0.375
Batch 90: C2_loss=0.404, C1_acc=1.000, C2_acc=0.750
Batch 100: C2_loss=0.535, C1_acc=1.000, C2_acc=0.625
Batch 110: C2_loss=0.298, C1_acc=1.000, C2_acc=1.000
Batch 120: C2_loss=1.060, C1_acc=1.000, C2_acc=0.625
Batch 130: C2_loss=0.443, C1_acc=1.000, C2_acc=0.875
Batch 140: C2_loss=0.649, C1_acc=1.000, C2_acc=0.625
Batch 150: C2_loss=0.356, C1_acc=1.000, C2_acc=0.875
Batch 160: C2_loss=0.776, C1_acc=1.000, C2_acc=0.750
Batch 170: C2_loss=0.256, C1_acc=1.000, C2_acc=1.000
Batch 180: C2_loss=1.048, C1_acc=1.000, C2_acc=0.750
Batch 190: C2_loss=0.148, C1_acc=1.000, C2_acc=1.000
Batch 200: C2_loss=0.560, C1_acc=1.000, C2_acc=0.625
Batch 210: C2_loss=0.497, C1_acc=1.000, C2_acc=0.750
Batch 220: C2_loss=0.799, C1_acc=1.000, C2_acc=0.750
Batch 230: C2_loss=0.331, C1_acc=1.000, C2_acc=0.875
Batch 240: C2_loss=0.379, C1_acc=1.000, C2_acc=0.750
Batch 250: C2_loss=0.773, C1_acc=1.000, C2_acc=0.750
Batch 260: C2_loss=0.336, C1_acc=1.000, C2_acc=0.875
Batch 270: C2_loss=0.408, C1_acc=1.000, C2_acc=0.750
Batch 280: C2_loss=0.344, C1_acc=1.000, C2_acc=0.875
Batch 290: C2_loss=0.404, C1_acc=1.000, C2_acc=0.875
Batch 300: C2_loss=1.116, C1_acc=1.000, C2_acc=0.625
Batch 310: C2_loss=1.019, C1_acc=1.000, C2_acc=0.375
Batch 320: C2_loss=0.589, C1_acc=1.000, C2_acc=0.625
Batch 330: C2_loss=0.856, C1_acc=1.000, C2_acc=0.750
Batch 340: C2_loss=0.492, C1_acc=1.000, C2_acc=0.875
Batch 350: C2_loss=0.711, C1_acc=1.000, C2_acc=0.750
Batch 360: C2_loss=0.602, C1_acc=1.000, C2_acc=0.875
Batch 370: C2_loss=1.237, C1_acc=1.000, C2_acc=0.375
Batch 380: C2_loss=0.415, C1_acc=1.000, C2_acc=0.875
Batch 390: C2_loss=0.429, C1_acc=1.000, C2_acc=0.750
Batch 400: C2_loss=0.672, C1_acc=1.000, C2_acc=0.625
Batch 410: C2_loss=1.173, C1_acc=1.000, C2_acc=0.750
Batch 420: C2_loss=0.513, C1_acc=1.000, C2_acc=0.750
Batch 430: C2_loss=0.852, C1_acc=1.000, C2_acc=0.625
Batch 440: C2_loss=0.600, C1_acc=1.000, C2_acc=0.875
Batch 450: C2_loss=0.692, C1_acc=1.000, C2_acc=0.750
Batch 460: C2_loss=0.350, C1_acc=1.000, C2_acc=0.875
Batch 470: C2_loss=0.240, C1_acc=1.000, C2_acc=0.875
Batch 480: C2_loss=1.057, C1_acc=1.000, C2_acc=0.875
Batch 490: C2_loss=0.292, C1_acc=1.000, C2_acc=0.875
Batch 500: C2_loss=0.570, C1_acc=1.000, C2_acc=0.625
Batch 510: C2_loss=0.584, C1_acc=1.000, C2_acc=0.750
Batch 520: C2_loss=0.292, C1_acc=1.000, C2_acc=0.875
Batch 530: C2_loss=0.608, C1_acc=1.000, C2_acc=0.750
Batch 540: C2_loss=0.140, C1_acc=1.000, C2_acc=1.000
Batch 550: C2_loss=0.294, C1_acc=1.000, C2_acc=0.875
Batch 560: C2_loss=0.674, C1_acc=1.000, C2_acc=0.500
Batch 570: C2_loss=0.785, C1_acc=1.000, C2_acc=0.625
Batch 580: C2_loss=0.305, C1_acc=1.000, C2_acc=0.875
Batch 590: C2_loss=0.232, C1_acc=1.000, C2_acc=0.875
Batch 600: C2_loss=0.107, C1_acc=1.000, C2_acc=1.000
Batch 610: C2_loss=0.062, C1_acc=1.000, C2_acc=1.000
Batch 620: C2_loss=0.288, C1_acc=1.000, C2_acc=0.875
Batch 630: C2_loss=0.430, C1_acc=1.000, C2_acc=0.875
Batch 640: C2_loss=0.611, C1_acc=1.000, C2_acc=0.750
Batch 650: C2_loss=0.671, C1_acc=1.000, C2_acc=0.750
Batch 660: C2_loss=0.451, C1_acc=1.000, C2_acc=0.625
Batch 670: C2_loss=0.630, C1_acc=1.000, C2_acc=0.750
Batch 680: C2_loss=0.218, C1_acc=1.000, C2_acc=1.000
Batch 690: C2_loss=0.234, C1_acc=1.000, C2_acc=1.000
Batch 700: C2_loss=0.782, C1_acc=1.000, C2_acc=0.750
Batch 710: C2_loss=1.164, C1_acc=1.000, C2_acc=0.500
Epoch 5 Summary:
  Stage: warmup
  💾 Checkpoint saved
\n=== EPOCH 6/50 ===
ADVERSARIAL STAGE - C2 vs Generator
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 713, in <module>
    main()
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 627, in main
    metrics = trainer.train_step_adversarial(batch, intensity, mask_fraction)
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 491, in train_step_adversarial
    wm_latent, wm_skip = self.generate_watermark(img, labels, intensity)
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 404, in generate_watermark
    wm_latent, wm_skip = self.watermark_gen(img, intensity)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py", line 179, in forward
    e1 = self.enc1(x)  # 512x512 -> 512x512
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
