Starting Fixed Adversarial Watermarking Training...
Loading models...
Loading C1 model...
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(C1_PATH, map_location=DEVICE)
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(AE_PATH, map_location=DEVICE)
Starting training...
\n=== EPOCH 1/5 ===
WARMUP STAGE - Training C2 normally
ðŸ”„ Creating dataloader for warmup stage...
ðŸ“¦ Creating dataloader: warmup stage, batch_size=64
Batch 0: C2_loss=1.904, C1_acc=0.969, C2_acc=0.234
Batch 10: C2_loss=1.499, C1_acc=0.984, C2_acc=0.453
Batch 20: C2_loss=1.095, C1_acc=1.000, C2_acc=0.594
Batch 30: C2_loss=1.071, C1_acc=1.000, C2_acc=0.500
Batch 40: C2_loss=1.041, C1_acc=1.000, C2_acc=0.547
Batch 50: C2_loss=0.821, C1_acc=1.000, C2_acc=0.656
Batch 60: C2_loss=1.156, C1_acc=1.000, C2_acc=0.578
Batch 70: C2_loss=0.703, C1_acc=1.000, C2_acc=0.688
Batch 80: C2_loss=0.589, C1_acc=1.000, C2_acc=0.734
Epoch 1 Summary:
  Stage: warmup
  C1 Performance: clean=0.998
  C2 Performance: clean=0.583
\n=== EPOCH 2/5 ===
ADVERSARIAL STAGE - C2 vs Generator
ðŸ”„ Creating dataloader for adversarial stage...
ðŸ“¦ Creating dataloader: adversarial stage, batch_size=16
Batch 0: C2_loss=26.967, Gen_loss=1.482
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.125
  Quality: L1=0.0003
  Losses: Misclassify=6.578, Detection=-7.015, Fooling=1.852
Batch 10: C2_loss=7.198, Gen_loss=1.160
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.562
  Quality: L1=0.0002
  Losses: Misclassify=2.170, Detection=-1.181, Fooling=1.450
Batch 20: C2_loss=8.286, Gen_loss=1.218
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.250
  Quality: L1=0.0004
  Losses: Misclassify=2.009, Detection=-2.176, Fooling=1.522
Batch 30: C2_loss=6.874, Gen_loss=1.567
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.312
  Quality: L1=0.0004
  Losses: Misclassify=2.019, Detection=-1.218, Fooling=1.958
Batch 40: C2_loss=8.056, Gen_loss=1.370
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.375
  Quality: L1=0.0005
  Losses: Misclassify=1.773, Detection=-2.415, Fooling=1.712
Batch 50: C2_loss=6.711, Gen_loss=1.219
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.188
  Quality: L1=0.0005
  Losses: Misclassify=1.850, Detection=-1.391, Fooling=1.524
Batch 60: C2_loss=7.041, Gen_loss=223719.109
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.188
  Quality: L1=0.0004
  Losses: Misclassify=1.787, Detection=-1.716, Fooling=1.515
Batch 70: C2_loss=6.868, Gen_loss=475946.781
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.188
  Quality: L1=0.0003
  Losses: Misclassify=1.579, Detection=-1.946, Fooling=1.354
Batch 80: C2_loss=7.157, Gen_loss=1.215
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.250
  Quality: L1=0.0003
  Losses: Misclassify=1.902, Detection=-1.601, Fooling=1.519
Batch 90: C2_loss=7.078, Gen_loss=1.150
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.375
  Quality: L1=0.0003
  Losses: Misclassify=1.934, Detection=-1.495, Fooling=1.437
Batch 100: C2_loss=5.327, Gen_loss=1.779
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.375
  Quality: L1=0.0003
  Losses: Misclassify=1.390, Detection=-1.235, Fooling=2.223
Batch 110: C2_loss=6.675, Gen_loss=1.217
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.500
  Quality: L1=0.0003
  Losses: Misclassify=1.759, Detection=-1.519, Fooling=1.522
Batch 120: C2_loss=6.307, Gen_loss=1.755
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.375
  Quality: L1=0.0004
  Losses: Misclassify=1.800, Detection=-1.205, Fooling=2.194
Batch 130: C2_loss=6.342, Gen_loss=1.113
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.250
  Quality: L1=0.0003
  Losses: Misclassify=1.729, Detection=-1.347, Fooling=1.392
Batch 140: C2_loss=5.815, Gen_loss=1.104
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.438
  Quality: L1=0.0003
  Losses: Misclassify=1.599, Detection=-1.212, Fooling=1.380
Batch 150: C2_loss=5.782, Gen_loss=1.409
  C1: clean=0.938, wm=0.938
  C2: clean=0.250, wm=0.250
  Quality: L1=0.0003
  Losses: Misclassify=1.498, Detection=-1.358, Fooling=1.761
Batch 160: C2_loss=5.102, Gen_loss=1.451
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.562
  Quality: L1=0.0003
  Losses: Misclassify=1.313, Detection=-1.213, Fooling=1.814
Batch 170: C2_loss=10.334, Gen_loss=1.340
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.438
  Quality: L1=0.0003
  Losses: Misclassify=3.299, Detection=-1.391, Fooling=1.675
Batch 180: C2_loss=5.215, Gen_loss=229267.594
  C1: clean=0.938, wm=0.938
  C2: clean=0.188, wm=0.438
  Quality: L1=0.0003
  Losses: Misclassify=1.335, Detection=-1.252, Fooling=1.683
Batch 190: C2_loss=5.710, Gen_loss=1.182
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.375
  Quality: L1=0.0004
  Losses: Misclassify=1.521, Detection=-1.272, Fooling=1.478
Batch 200: C2_loss=5.912, Gen_loss=1.154
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.188
  Quality: L1=0.0004
  Losses: Misclassify=1.417, Detection=-1.579, Fooling=1.443
Batch 210: C2_loss=5.152, Gen_loss=1.183
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.188
  Quality: L1=0.0004
  Losses: Misclassify=1.242, Detection=-1.364, Fooling=1.478
Batch 220: C2_loss=6.200, Gen_loss=1.143
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.125
  Quality: L1=0.0004
  Losses: Misclassify=1.660, Detection=-1.367, Fooling=1.428
Batch 230: C2_loss=5.546, Gen_loss=1.361
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.250
  Quality: L1=0.0004
  Losses: Misclassify=1.319, Detection=-1.499, Fooling=1.701
Batch 240: C2_loss=5.786, Gen_loss=1.523
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.500
  Quality: L1=0.0004
  Losses: Misclassify=1.551, Detection=-1.273, Fooling=1.904
Batch 250: C2_loss=5.879, Gen_loss=1.044
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.250
  Quality: L1=0.0004
  Losses: Misclassify=1.501, Detection=-1.417, Fooling=1.305
Batch 260: C2_loss=5.813, Gen_loss=1.256
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.375
  Quality: L1=0.0004
  Losses: Misclassify=1.549, Detection=-1.294, Fooling=1.571
Batch 270: C2_loss=5.941, Gen_loss=1.110
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.250
  Quality: L1=0.0004
  Losses: Misclassify=1.541, Detection=-1.392, Fooling=1.387
Batch 280: C2_loss=6.201, Gen_loss=1.057
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.312
  Quality: L1=0.0004
  Losses: Misclassify=1.708, Detection=-1.287, Fooling=1.322
Batch 290: C2_loss=5.799, Gen_loss=1.380
  C1: clean=1.000, wm=1.000
  C2: clean=0.562, wm=0.500
  Quality: L1=0.0004
  Losses: Misclassify=1.582, Detection=-1.228, Fooling=1.725
Batch 300: C2_loss=5.410, Gen_loss=1.265
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.500
  Quality: L1=0.0004
  Losses: Misclassify=1.377, Detection=-1.311, Fooling=1.582
Batch 310: C2_loss=5.994, Gen_loss=1.166
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.500
  Quality: L1=0.0004
  Losses: Misclassify=1.653, Detection=-1.241, Fooling=1.458
Batch 320: C2_loss=5.485, Gen_loss=1.137
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.500
  Quality: L1=0.0004
  Losses: Misclassify=1.458, Detection=-1.226, Fooling=1.421
Batch 330: C2_loss=6.354, Gen_loss=1.302
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.375
  Quality: L1=0.0004
  Losses: Misclassify=1.681, Detection=-1.434, Fooling=1.627
Batch 340: C2_loss=5.874, Gen_loss=1.165
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.562
  Quality: L1=0.0005
  Losses: Misclassify=1.485, Detection=-1.441, Fooling=1.456
Batch 350: C2_loss=5.570, Gen_loss=1.286
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.500
  Quality: L1=0.0008
  Losses: Misclassify=1.452, Detection=-1.294, Fooling=1.607
Epoch 2 Summary:
  Stage: adversarial
  C1 Performance: clean=0.998, wm=0.998
  C2 Performance: clean=0.350, wm=0.355
  Quality: L1=0.0004
  Watermark Signal: intensity=0.300, entropy_clean=0.000
  âš ï¸  C2 is not detecting watermarks well enough
\n=== EPOCH 3/5 ===
ADVERSARIAL STAGE - C2 vs Generator
Batch 0: C2_loss=5.657, Gen_loss=1.444
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.250
  Quality: L1=0.0009
  Losses: Misclassify=1.505, Detection=-1.263, Fooling=1.804
Batch 10: C2_loss=5.429, Gen_loss=1.796
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.500
  Quality: L1=0.0009
  Losses: Misclassify=1.484, Detection=-1.145, Fooling=2.245
Batch 20: C2_loss=5.695, Gen_loss=1.159
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.438
  Quality: L1=0.0017
  Losses: Misclassify=1.348, Detection=-1.549, Fooling=1.449
Batch 30: C2_loss=5.525, Gen_loss=1.138
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.125
  Quality: L1=0.0017
  Losses: Misclassify=1.329, Detection=-1.469, Fooling=1.423
Batch 40: C2_loss=6.225, Gen_loss=1.053
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.188
  Quality: L1=0.0020
  Losses: Misclassify=1.610, Detection=-1.467, Fooling=1.316
Batch 50: C2_loss=6.065, Gen_loss=1.289
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.375
  Quality: L1=0.0023
  Losses: Misclassify=1.592, Detection=-1.390, Fooling=1.611
Batch 60: C2_loss=5.666, Gen_loss=1.297
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.438
  Quality: L1=0.0024
  Losses: Misclassify=1.477, Detection=-1.316, Fooling=1.621
Batch 70: C2_loss=5.677, Gen_loss=1.160
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.500
  Quality: L1=0.0025
  Losses: Misclassify=1.492, Detection=-1.299, Fooling=1.450
Batch 80: C2_loss=6.070, Gen_loss=1.274
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.438
  Quality: L1=0.0027
  Losses: Misclassify=1.661, Detection=-1.279, Fooling=1.592
Batch 90: C2_loss=5.888, Gen_loss=205104.531
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.250
  Quality: L1=0.0026
  Losses: Misclassify=1.495, Detection=-1.434, Fooling=1.579
Batch 100: C2_loss=5.763, Gen_loss=1.086
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.312
  Quality: L1=0.0027
  Losses: Misclassify=1.498, Detection=-1.345, Fooling=1.358
Batch 110: C2_loss=5.914, Gen_loss=1.333
  C1: clean=1.000, wm=1.000
  C2: clean=0.562, wm=0.500
  Quality: L1=0.0025
  Losses: Misclassify=1.532, Detection=-1.389, Fooling=1.666
Batch 120: C2_loss=5.507, Gen_loss=1.362
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.562
  Quality: L1=0.0023
  Losses: Misclassify=1.428, Detection=-1.291, Fooling=1.702
Batch 130: C2_loss=5.726, Gen_loss=1.186
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.250
  Quality: L1=0.0028
  Losses: Misclassify=1.432, Detection=-1.431, Fooling=1.483
Batch 140: C2_loss=5.727, Gen_loss=1.167
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.250
  Quality: L1=0.0031
  Losses: Misclassify=1.370, Detection=-1.535, Fooling=1.459
Batch 150: C2_loss=6.540, Gen_loss=1.278
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.312
  Quality: L1=0.0033
  Losses: Misclassify=1.749, Detection=-1.445, Fooling=1.597
Batch 160: C2_loss=5.579, Gen_loss=1.217
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.250
  Quality: L1=0.0030
  Losses: Misclassify=1.404, Detection=-1.380, Fooling=1.521
Batch 170: C2_loss=5.605, Gen_loss=1.058
  C1: clean=0.938, wm=0.938
  C2: clean=0.500, wm=0.438
  Quality: L1=0.0031
  Losses: Misclassify=1.358, Detection=-1.474, Fooling=1.323
Batch 180: C2_loss=5.335, Gen_loss=1.111
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.375
  Quality: L1=0.0032
  Losses: Misclassify=1.287, Detection=-1.411, Fooling=1.389
Batch 190: C2_loss=5.758, Gen_loss=1.160
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.562
  Quality: L1=0.0030
  Losses: Misclassify=1.548, Detection=-1.260, Fooling=1.450
Batch 200: C2_loss=5.143, Gen_loss=1.218
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.500
  Quality: L1=0.0033
  Losses: Misclassify=1.319, Detection=-1.231, Fooling=1.522
Batch 210: C2_loss=5.734, Gen_loss=1.124
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.188
  Quality: L1=0.0032
  Losses: Misclassify=1.401, Detection=-1.487, Fooling=1.405
Batch 220: C2_loss=5.716, Gen_loss=1.136
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.188
  Quality: L1=0.0031
  Losses: Misclassify=1.366, Detection=-1.533, Fooling=1.419
Batch 230: C2_loss=5.496, Gen_loss=1.137
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.625
  Quality: L1=0.0033
  Losses: Misclassify=1.458, Detection=-1.233, Fooling=1.422
Batch 240: C2_loss=5.360, Gen_loss=1.224
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.438
  Quality: L1=0.0037
  Losses: Misclassify=1.364, Detection=-1.301, Fooling=1.529
Batch 250: C2_loss=5.608, Gen_loss=1.195
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.312
  Quality: L1=0.0037
  Losses: Misclassify=1.452, Detection=-1.318, Fooling=1.493
Batch 260: C2_loss=5.864, Gen_loss=1.119
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.250
  Quality: L1=0.0042
  Losses: Misclassify=1.479, Detection=-1.444, Fooling=1.399
Batch 270: C2_loss=5.720, Gen_loss=1.142
  C1: clean=1.000, wm=1.000
  C2: clean=0.562, wm=0.438
  Quality: L1=0.0041
  Losses: Misclassify=1.533, Detection=-1.257, Fooling=1.428
Batch 280: C2_loss=5.733, Gen_loss=1.140
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.562
  Quality: L1=0.0041
  Losses: Misclassify=1.517, Detection=-1.294, Fooling=1.425
Batch 290: C2_loss=5.291, Gen_loss=1.196
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.250
  Quality: L1=0.0038
  Losses: Misclassify=1.305, Detection=-1.353, Fooling=1.495
Batch 300: C2_loss=5.485, Gen_loss=1.111
  C1: clean=0.938, wm=1.000
  C2: clean=0.688, wm=0.750
  Quality: L1=0.0041
  Losses: Misclassify=1.425, Detection=-1.282, Fooling=1.389
Batch 310: C2_loss=5.602, Gen_loss=1.212
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.625
  Quality: L1=0.0041
  Losses: Misclassify=1.481, Detection=-1.267, Fooling=1.515
Batch 320: C2_loss=5.875, Gen_loss=1.090
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.500
  Quality: L1=0.0040
  Losses: Misclassify=1.542, Detection=-1.347, Fooling=1.362
Batch 330: C2_loss=5.556, Gen_loss=1.087
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.188
  Quality: L1=0.0039
  Losses: Misclassify=1.391, Detection=-1.386, Fooling=1.358
Batch 340: C2_loss=5.714, Gen_loss=1.189
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.312
  Quality: L1=0.0034
  Losses: Misclassify=1.412, Detection=-1.457, Fooling=1.486
Batch 350: C2_loss=6.134, Gen_loss=1.113
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.375
  Quality: L1=0.0036
  Losses: Misclassify=1.634, Detection=-1.366, Fooling=1.390
Epoch 3 Summary:
  Stage: adversarial
  C1 Performance: clean=0.998, wm=0.998
  C2 Performance: clean=0.372, wm=0.373
  Quality: L1=0.0030
  Watermark Signal: intensity=0.300, entropy_clean=0.000
  âš ï¸  C2 is not detecting watermarks well enough
   C2 watermark detection too low (0.373), increased intensity to 0.330
   C2 Distinction Score: 0.001 (target: >0.5)
   C2 cannot distinguish watermarked from clean! Increasing watermark strength...
\n=== EPOCH 4/5 ===
ADVERSARIAL STAGE - C2 vs Generator
Batch 0: C2_loss=6.054, Gen_loss=1.166
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.312
  Quality: L1=0.0047
  Losses: Misclassify=1.576, Detection=-1.410, Fooling=1.458
Batch 10: C2_loss=5.396, Gen_loss=266311.594
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.375
  Quality: L1=0.0027
  Losses: Misclassify=1.331, Detection=-1.380, Fooling=1.355
Batch 20: C2_loss=5.451, Gen_loss=1.312
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.375
  Quality: L1=0.0026
  Losses: Misclassify=1.372, Detection=-1.347, Fooling=1.640
Batch 30: C2_loss=5.969, Gen_loss=1.149
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.250
  Quality: L1=0.0018
  Losses: Misclassify=1.562, Detection=-1.376, Fooling=1.436
Batch 40: C2_loss=5.442, Gen_loss=1.068
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.375
  Quality: L1=0.0021
  Losses: Misclassify=1.348, Detection=-1.381, Fooling=1.335
Batch 50: C2_loss=5.697, Gen_loss=1.178
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.375
  Quality: L1=0.0018
  Losses: Misclassify=1.468, Detection=-1.352, Fooling=1.473
Batch 60: C2_loss=5.969, Gen_loss=1.109
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.250
  Quality: L1=0.0021
  Losses: Misclassify=1.569, Detection=-1.365, Fooling=1.386
Batch 70: C2_loss=5.395, Gen_loss=1.146
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.312
  Quality: L1=0.0024
  Losses: Misclassify=1.359, Detection=-1.331, Fooling=1.432
Batch 80: C2_loss=5.843, Gen_loss=1.172
  C1: clean=1.000, wm=1.000
  C2: clean=0.562, wm=0.500
  Quality: L1=0.0019
  Losses: Misclassify=1.586, Detection=-1.253, Fooling=1.465
Batch 90: C2_loss=5.977, Gen_loss=1.123
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.250
  Quality: L1=0.0014
  Losses: Misclassify=1.504, Detection=-1.478, Fooling=1.403
Batch 100: C2_loss=5.482, Gen_loss=1.097
  C1: clean=1.000, wm=1.000
  C2: clean=0.062, wm=0.062
  Quality: L1=0.0011
  Losses: Misclassify=1.299, Detection=-1.489, Fooling=1.371
Batch 110: C2_loss=5.695, Gen_loss=1.091
  C1: clean=1.000, wm=1.000
  C2: clean=0.062, wm=0.188
  Quality: L1=0.0006
  Losses: Misclassify=1.361, Detection=-1.528, Fooling=1.364
Batch 120: C2_loss=5.925, Gen_loss=1.159
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.250
  Quality: L1=0.0007
  Losses: Misclassify=1.521, Detection=-1.415, Fooling=1.448
Batch 130: C2_loss=5.590, Gen_loss=1.093
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.562
  Quality: L1=0.0006
  Losses: Misclassify=1.424, Detection=-1.354, Fooling=1.366
Batch 140: C2_loss=5.813, Gen_loss=1.178
  C1: clean=1.000, wm=1.000
  C2: clean=0.562, wm=0.562
  Quality: L1=0.0007
  Losses: Misclassify=1.511, Detection=-1.358, Fooling=1.472
Batch 150: C2_loss=5.449, Gen_loss=1.109
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.500
  Quality: L1=0.0009
  Losses: Misclassify=1.398, Detection=-1.303, Fooling=1.386
Batch 160: C2_loss=5.422, Gen_loss=1.168
  C1: clean=1.000, wm=1.000
  C2: clean=0.562, wm=0.500
  Quality: L1=0.0008
  Losses: Misclassify=1.388, Detection=-1.302, Fooling=1.460
Batch 170: C2_loss=5.552, Gen_loss=1.048
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.062
  Quality: L1=0.0005
  Losses: Misclassify=1.317, Detection=-1.507, Fooling=1.310
Batch 180: C2_loss=5.796, Gen_loss=1.103
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.375
  Quality: L1=0.0005
  Losses: Misclassify=1.495, Detection=-1.373, Fooling=1.379
Batch 190: C2_loss=5.687, Gen_loss=1.149
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.375
  Quality: L1=0.0005
  Losses: Misclassify=1.475, Detection=-1.333, Fooling=1.437
Batch 200: C2_loss=5.410, Gen_loss=1.282
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.375
  Quality: L1=0.0008
  Losses: Misclassify=1.351, Detection=-1.354, Fooling=1.603
Batch 210: C2_loss=5.390, Gen_loss=1.087
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.438
  Quality: L1=0.0014
  Losses: Misclassify=1.359, Detection=-1.328, Fooling=1.358
Batch 220: C2_loss=5.652, Gen_loss=1.208
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.312
  Quality: L1=0.0022
  Losses: Misclassify=1.440, Detection=-1.368, Fooling=1.509
Batch 230: C2_loss=5.319, Gen_loss=1.168
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.312
  Quality: L1=0.0023
  Losses: Misclassify=1.337, Detection=-1.318, Fooling=1.460
Batch 240: C2_loss=5.486, Gen_loss=1.074
  C1: clean=1.000, wm=1.000
  C2: clean=0.625, wm=0.500
  Quality: L1=0.0022
  Losses: Misclassify=1.405, Detection=-1.316, Fooling=1.342
Batch 250: C2_loss=5.511, Gen_loss=1.107
  C1: clean=0.938, wm=0.938
  C2: clean=0.625, wm=0.438
  Quality: L1=0.0021
  Losses: Misclassify=1.433, Detection=-1.286, Fooling=1.383
Batch 260: C2_loss=5.527, Gen_loss=1.240
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.562
  Quality: L1=0.0020
  Losses: Misclassify=1.467, Detection=-1.239, Fooling=1.550
Batch 270: C2_loss=5.721, Gen_loss=1.087
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.375
  Quality: L1=0.0014
  Losses: Misclassify=1.445, Detection=-1.406, Fooling=1.359
Batch 280: C2_loss=5.746, Gen_loss=1.235
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.250
  Quality: L1=0.0007
  Losses: Misclassify=1.427, Detection=-1.452, Fooling=1.543
Batch 290: C2_loss=6.123, Gen_loss=1.067
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.000
  Quality: L1=0.0005
  Losses: Misclassify=1.546, Detection=-1.505, Fooling=1.333
Batch 300: C2_loss=5.768, Gen_loss=1.079
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.438
  Quality: L1=0.0005
  Losses: Misclassify=1.389, Detection=-1.530, Fooling=1.349
Batch 310: C2_loss=5.669, Gen_loss=1.067
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.312
  Quality: L1=0.0005
  Losses: Misclassify=1.413, Detection=-1.425, Fooling=1.334
Batch 320: C2_loss=5.760, Gen_loss=1.205
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.188
  Quality: L1=0.0005
  Losses: Misclassify=1.435, Detection=-1.449, Fooling=1.506
Batch 330: C2_loss=5.527, Gen_loss=248529.875
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.375
  Quality: L1=0.0006
  Losses: Misclassify=1.428, Detection=-1.305, Fooling=1.483
Batch 340: C2_loss=6.062, Gen_loss=1.077
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.125
  Quality: L1=0.0006
  Losses: Misclassify=1.451, Detection=-1.623, Fooling=1.347
Batch 350: C2_loss=5.664, Gen_loss=1.210
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.438
  Quality: L1=0.0008
  Losses: Misclassify=1.519, Detection=-1.245, Fooling=1.513
Epoch 4 Summary:
  Stage: adversarial
  C1 Performance: clean=0.998, wm=0.998
  C2 Performance: clean=0.329, wm=0.323
  Quality: L1=0.0013
  Watermark Signal: intensity=0.396, entropy_clean=0.000
  âš ï¸  C2 is not detecting watermarks well enough
   C2 watermark detection too low (0.323), increased intensity to 0.436
   C2 Distinction Score: -0.006 (target: >0.5)
   C2 cannot distinguish watermarked from clean! Increasing watermark strength...
\n=== EPOCH 5/5 ===
ADVERSARIAL STAGE - C2 vs Generator
Batch 0: C2_loss=5.424, Gen_loss=1.197
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.312
  Quality: L1=0.0012
  Losses: Misclassify=1.360, Detection=-1.349, Fooling=1.496
Batch 10: C2_loss=5.410, Gen_loss=1.381
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.375
  Quality: L1=0.0008
  Losses: Misclassify=1.360, Detection=-1.340, Fooling=1.726
Batch 20: C2_loss=5.779, Gen_loss=1.123
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.188
  Quality: L1=0.0007
  Losses: Misclassify=1.431, Detection=-1.468, Fooling=1.404
Batch 30: C2_loss=5.597, Gen_loss=1.139
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.250
  Quality: L1=0.0006
  Losses: Misclassify=1.440, Detection=-1.332, Fooling=1.424
Batch 40: C2_loss=5.464, Gen_loss=1.180
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.375
  Quality: L1=0.0006
  Losses: Misclassify=1.412, Detection=-1.289, Fooling=1.475
Batch 50: C2_loss=5.690, Gen_loss=1.117
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.188
  Quality: L1=0.0006
  Losses: Misclassify=1.401, Detection=-1.457, Fooling=1.396
Batch 60: C2_loss=5.555, Gen_loss=1.156
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.250
  Quality: L1=0.0008
  Losses: Misclassify=1.396, Detection=-1.377, Fooling=1.445
Batch 70: C2_loss=5.573, Gen_loss=1.187
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.250
  Quality: L1=0.0008
  Losses: Misclassify=1.418, Detection=-1.352, Fooling=1.484
Batch 80: C2_loss=5.484, Gen_loss=1.173
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.375
  Quality: L1=0.0009
  Losses: Misclassify=1.407, Detection=-1.312, Fooling=1.466
Batch 90: C2_loss=5.696, Gen_loss=1.080
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.188
  Quality: L1=0.0012
  Losses: Misclassify=1.409, Detection=-1.450, Fooling=1.350
Batch 100: C2_loss=5.555, Gen_loss=1.074
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.125
  Quality: L1=0.0016
  Losses: Misclassify=1.372, Detection=-1.416, Fooling=1.343
Batch 110: C2_loss=5.292, Gen_loss=1.160
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.375
  Quality: L1=0.0021
  Losses: Misclassify=1.300, Detection=-1.361, Fooling=1.450
Batch 120: C2_loss=5.660, Gen_loss=1.091
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.312
  Quality: L1=0.0015
  Losses: Misclassify=1.449, Detection=-1.359, Fooling=1.363
Batch 130: C2_loss=5.670, Gen_loss=1.197
  C1: clean=0.938, wm=0.938
  C2: clean=0.375, wm=0.375
  Quality: L1=0.0017
  Losses: Misclassify=1.490, Detection=-1.296, Fooling=1.496
Batch 140: C2_loss=5.587, Gen_loss=1.057
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.375
  Quality: L1=0.0012
  Losses: Misclassify=1.412, Detection=-1.372, Fooling=1.321
Batch 150: C2_loss=5.799, Gen_loss=1.045
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.188
  Quality: L1=0.0018
  Losses: Misclassify=1.471, Detection=-1.415, Fooling=1.306
Batch 160: C2_loss=5.755, Gen_loss=1.209
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.250
  Quality: L1=0.0019
  Losses: Misclassify=1.431, Detection=-1.452, Fooling=1.511
Batch 170: C2_loss=5.897, Gen_loss=1.144
  C1: clean=1.000, wm=1.000
  C2: clean=0.438, wm=0.188
  Quality: L1=0.0019
  Losses: Misclassify=1.496, Detection=-1.438, Fooling=1.430
Batch 180: C2_loss=5.433, Gen_loss=1.098
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.312
  Quality: L1=0.0014
  Losses: Misclassify=1.356, Detection=-1.362, Fooling=1.372
Batch 190: C2_loss=5.472, Gen_loss=1.103
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.438
  Quality: L1=0.0008
  Losses: Misclassify=1.400, Detection=-1.315, Fooling=1.378
Batch 200: C2_loss=5.637, Gen_loss=1.161
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.375
  Quality: L1=0.0008
  Losses: Misclassify=1.427, Detection=-1.379, Fooling=1.452
Batch 210: C2_loss=5.441, Gen_loss=1.117
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.375
  Quality: L1=0.0008
  Losses: Misclassify=1.383, Detection=-1.322, Fooling=1.397
Batch 220: C2_loss=5.784, Gen_loss=1.146
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.125
  Quality: L1=0.0009
  Losses: Misclassify=1.430, Detection=-1.473, Fooling=1.432
Batch 230: C2_loss=5.837, Gen_loss=1.176
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.250
  Quality: L1=0.0015
  Losses: Misclassify=1.502, Detection=-1.388, Fooling=1.470
Batch 240: C2_loss=5.556, Gen_loss=1.135
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.250
  Quality: L1=0.0014
  Losses: Misclassify=1.411, Detection=-1.352, Fooling=1.419
Batch 250: C2_loss=5.599, Gen_loss=1.195
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.375
  Quality: L1=0.0013
  Losses: Misclassify=1.481, Detection=-1.265, Fooling=1.494
Batch 260: C2_loss=5.444, Gen_loss=1.145
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.375
  Quality: L1=0.0017
  Losses: Misclassify=1.380, Detection=-1.329, Fooling=1.431
Batch 270: C2_loss=5.600, Gen_loss=1.104
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.375
  Quality: L1=0.0013
  Losses: Misclassify=1.446, Detection=-1.323, Fooling=1.380
Batch 280: C2_loss=5.336, Gen_loss=1.147
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.188
  Quality: L1=0.0014
  Losses: Misclassify=1.300, Detection=-1.390, Fooling=1.434
Batch 290: C2_loss=5.833, Gen_loss=1.137
  C1: clean=1.000, wm=1.000
  C2: clean=0.375, wm=0.312
  Quality: L1=0.0016
  Losses: Misclassify=1.514, Detection=-1.365, Fooling=1.421
Batch 300: C2_loss=5.336, Gen_loss=1.173
  C1: clean=1.000, wm=1.000
  C2: clean=0.188, wm=0.375
  Quality: L1=0.0013
  Losses: Misclassify=1.345, Detection=-1.317, Fooling=1.466
Batch 310: C2_loss=5.744, Gen_loss=1.166
  C1: clean=1.000, wm=1.000
  C2: clean=0.312, wm=0.250
  Quality: L1=0.0016
  Losses: Misclassify=1.476, Detection=-1.369, Fooling=1.458
Batch 320: C2_loss=5.656, Gen_loss=1.123
  C1: clean=1.000, wm=1.000
  C2: clean=0.562, wm=0.188
  Quality: L1=0.0013
  Losses: Misclassify=1.395, Detection=-1.446, Fooling=1.404
Batch 330: C2_loss=5.643, Gen_loss=1.161
  C1: clean=1.000, wm=1.000
  C2: clean=0.250, wm=0.375
  Quality: L1=0.0012
  Losses: Misclassify=1.447, Detection=-1.350, Fooling=1.451
Batch 340: C2_loss=5.650, Gen_loss=1.150
  C1: clean=1.000, wm=1.000
  C2: clean=0.500, wm=0.438
  Quality: L1=0.0014
  Losses: Misclassify=1.449, Detection=-1.351, Fooling=1.438
Batch 350: C2_loss=5.572, Gen_loss=1.188
  C1: clean=1.000, wm=1.000
  C2: clean=0.125, wm=0.312
  Quality: L1=0.0015
  Losses: Misclassify=1.367, Detection=-1.436, Fooling=1.485
Epoch 5 Summary:
  Stage: adversarial
  C1 Performance: clean=0.998, wm=0.998
  C2 Performance: clean=0.304, wm=0.318
  Quality: L1=0.0012
  Watermark Signal: intensity=0.523, entropy_clean=0.000
  âš ï¸  C2 is not detecting watermarks well enough
  ðŸ’¾ Checkpoint saved
   C2 watermark detection too low (0.318), increased intensity to 0.575
   C2 Distinction Score: 0.013 (target: >0.5)
   C2 cannot distinguish watermarked from clean! Increasing watermark strength...
\nðŸŽ‰ Training completed! Results saved to /teamspace/studios/this_studio/unetMRI/output/csv_logs_FIXED/final_training_results.csv
\nFinal Performance:
  C1: clean=0.998, wm=0.998
  C2: clean=0.304, wm=0.318
