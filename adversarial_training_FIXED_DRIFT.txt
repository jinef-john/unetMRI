Starting Fixed Adversarial Watermarking Training...
Loading models...
Loading C1 model...
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:237: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(C1_PATH, map_location=DEVICE)
/teamspace/studios/this_studio/unetMRI/src/watermarking/MRI_Watermark_Embedding_CYCLES_FIXED_ADVERSARIAL.py:259: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(AE_PATH, map_location=DEVICE)
Starting training...
\n=== EPOCH 1/3 ===
WARMUP STAGE - Training C2 normally
ðŸ”„ Creating dataloader for warmup stage...
ðŸ“¦ Creating dataloader: warmup stage, batch_size=8
Batch 0: C2_loss=7.439, C1_acc=1.000, C2_acc=0.375
Batch 10: C2_loss=1.822, C1_acc=1.000, C2_acc=0.500
Batch 20: C2_loss=3.372, C1_acc=1.000, C2_acc=0.375
Batch 30: C2_loss=3.446, C1_acc=1.000, C2_acc=0.125
Batch 40: C2_loss=5.141, C1_acc=1.000, C2_acc=0.500
Batch 50: C2_loss=3.390, C1_acc=1.000, C2_acc=0.375
Batch 60: C2_loss=1.349, C1_acc=1.000, C2_acc=0.500
Batch 70: C2_loss=1.777, C1_acc=1.000, C2_acc=0.250
Batch 80: C2_loss=2.197, C1_acc=1.000, C2_acc=0.250
Batch 90: C2_loss=1.130, C1_acc=1.000, C2_acc=0.375
Batch 100: C2_loss=1.394, C1_acc=1.000, C2_acc=0.250
Batch 110: C2_loss=1.978, C1_acc=1.000, C2_acc=0.500
Batch 120: C2_loss=1.022, C1_acc=1.000, C2_acc=0.625
Batch 130: C2_loss=1.887, C1_acc=1.000, C2_acc=0.250
Batch 140: C2_loss=0.995, C1_acc=1.000, C2_acc=0.500
Batch 150: C2_loss=1.398, C1_acc=1.000, C2_acc=0.375
Batch 160: C2_loss=1.108, C1_acc=1.000, C2_acc=0.625
Batch 170: C2_loss=1.703, C1_acc=1.000, C2_acc=0.500
Batch 180: C2_loss=1.205, C1_acc=1.000, C2_acc=0.375
Batch 190: C2_loss=1.301, C1_acc=1.000, C2_acc=0.625
Batch 200: C2_loss=1.090, C1_acc=1.000, C2_acc=0.625
Batch 210: C2_loss=2.172, C1_acc=1.000, C2_acc=0.375
Batch 220: C2_loss=1.893, C1_acc=1.000, C2_acc=0.250
Batch 230: C2_loss=1.540, C1_acc=1.000, C2_acc=0.250
Batch 240: C2_loss=0.870, C1_acc=1.000, C2_acc=0.500
Batch 250: C2_loss=1.718, C1_acc=1.000, C2_acc=0.500
Batch 260: C2_loss=0.752, C1_acc=1.000, C2_acc=0.750
Batch 270: C2_loss=1.716, C1_acc=1.000, C2_acc=0.500
Batch 280: C2_loss=1.118, C1_acc=1.000, C2_acc=0.250
Batch 290: C2_loss=0.899, C1_acc=1.000, C2_acc=0.750
Batch 300: C2_loss=1.118, C1_acc=1.000, C2_acc=0.500
Batch 310: C2_loss=0.888, C1_acc=1.000, C2_acc=0.500
Batch 320: C2_loss=0.916, C1_acc=1.000, C2_acc=0.500
Batch 330: C2_loss=0.905, C1_acc=1.000, C2_acc=0.500
Batch 340: C2_loss=1.867, C1_acc=1.000, C2_acc=0.375
Batch 350: C2_loss=1.097, C1_acc=1.000, C2_acc=0.500
Batch 360: C2_loss=1.373, C1_acc=1.000, C2_acc=0.500
Batch 370: C2_loss=1.288, C1_acc=1.000, C2_acc=0.625
Batch 380: C2_loss=1.111, C1_acc=1.000, C2_acc=0.500
Batch 390: C2_loss=0.869, C1_acc=1.000, C2_acc=0.625
Batch 400: C2_loss=1.233, C1_acc=1.000, C2_acc=0.625
Batch 410: C2_loss=0.954, C1_acc=1.000, C2_acc=0.750
Batch 420: C2_loss=1.192, C1_acc=1.000, C2_acc=0.500
Batch 430: C2_loss=0.809, C1_acc=1.000, C2_acc=0.375
Batch 440: C2_loss=0.823, C1_acc=1.000, C2_acc=0.750
Batch 450: C2_loss=0.782, C1_acc=1.000, C2_acc=0.375
Batch 460: C2_loss=0.727, C1_acc=1.000, C2_acc=0.625
Batch 470: C2_loss=0.737, C1_acc=1.000, C2_acc=0.625
Batch 480: C2_loss=0.856, C1_acc=1.000, C2_acc=0.500
Batch 490: C2_loss=2.485, C1_acc=1.000, C2_acc=0.375
Batch 500: C2_loss=1.632, C1_acc=1.000, C2_acc=0.250
Batch 510: C2_loss=0.789, C1_acc=1.000, C2_acc=0.625
Batch 520: C2_loss=0.508, C1_acc=1.000, C2_acc=1.000
Batch 530: C2_loss=0.762, C1_acc=1.000, C2_acc=0.750
Batch 540: C2_loss=2.458, C1_acc=1.000, C2_acc=0.500
Batch 550: C2_loss=1.579, C1_acc=1.000, C2_acc=0.375
Batch 560: C2_loss=1.147, C1_acc=1.000, C2_acc=0.500
Batch 570: C2_loss=0.816, C1_acc=1.000, C2_acc=0.500
Batch 580: C2_loss=1.052, C1_acc=1.000, C2_acc=0.375
Batch 590: C2_loss=1.350, C1_acc=1.000, C2_acc=0.625
Batch 600: C2_loss=0.382, C1_acc=1.000, C2_acc=0.875
Batch 610: C2_loss=1.187, C1_acc=1.000, C2_acc=0.375
Batch 620: C2_loss=1.038, C1_acc=1.000, C2_acc=0.500
Batch 630: C2_loss=1.911, C1_acc=1.000, C2_acc=0.500
Batch 640: C2_loss=2.186, C1_acc=1.000, C2_acc=0.375
Batch 650: C2_loss=0.715, C1_acc=1.000, C2_acc=0.750
Batch 660: C2_loss=0.571, C1_acc=1.000, C2_acc=0.750
Batch 670: C2_loss=1.149, C1_acc=1.000, C2_acc=0.625
Batch 680: C2_loss=0.986, C1_acc=1.000, C2_acc=0.625
Batch 690: C2_loss=0.693, C1_acc=1.000, C2_acc=0.625
Batch 700: C2_loss=1.528, C1_acc=1.000, C2_acc=0.500
Batch 710: C2_loss=0.685, C1_acc=1.000, C2_acc=0.750
Epoch 1 Summary:
  Stage: warmup
\n=== EPOCH 2/3 ===
WARMUP STAGE - Training C2 normally
Batch 0: C2_loss=0.789, C1_acc=1.000, C2_acc=0.750
Batch 10: C2_loss=0.864, C1_acc=1.000, C2_acc=0.750
Batch 20: C2_loss=0.598, C1_acc=1.000, C2_acc=0.750
Batch 30: C2_loss=0.815, C1_acc=1.000, C2_acc=0.750
Batch 40: C2_loss=0.987, C1_acc=1.000, C2_acc=0.625
Batch 50: C2_loss=0.803, C1_acc=1.000, C2_acc=0.750
Batch 60: C2_loss=0.903, C1_acc=1.000, C2_acc=0.625
Batch 70: C2_loss=1.048, C1_acc=1.000, C2_acc=0.625
Batch 80: C2_loss=1.143, C1_acc=1.000, C2_acc=0.375
Batch 90: C2_loss=0.900, C1_acc=1.000, C2_acc=0.750
Batch 100: C2_loss=0.786, C1_acc=1.000, C2_acc=0.625
Batch 110: C2_loss=1.710, C1_acc=1.000, C2_acc=0.250
Batch 120: C2_loss=1.136, C1_acc=1.000, C2_acc=0.500
Batch 130: C2_loss=0.996, C1_acc=1.000, C2_acc=0.500
Batch 140: C2_loss=1.374, C1_acc=1.000, C2_acc=0.500
Batch 150: C2_loss=1.177, C1_acc=1.000, C2_acc=0.375
Batch 160: C2_loss=1.598, C1_acc=1.000, C2_acc=0.375
Batch 170: C2_loss=1.156, C1_acc=1.000, C2_acc=0.625
Batch 180: C2_loss=1.457, C1_acc=1.000, C2_acc=0.375
Batch 190: C2_loss=1.084, C1_acc=1.000, C2_acc=0.500
Batch 200: C2_loss=1.102, C1_acc=1.000, C2_acc=0.375
Batch 210: C2_loss=0.725, C1_acc=1.000, C2_acc=0.625
Batch 220: C2_loss=1.086, C1_acc=1.000, C2_acc=0.625
Batch 230: C2_loss=0.827, C1_acc=1.000, C2_acc=0.500
Batch 240: C2_loss=0.690, C1_acc=1.000, C2_acc=0.875
Batch 250: C2_loss=0.869, C1_acc=1.000, C2_acc=0.625
Batch 260: C2_loss=0.917, C1_acc=1.000, C2_acc=0.500
Batch 270: C2_loss=0.437, C1_acc=1.000, C2_acc=1.000
Batch 280: C2_loss=0.458, C1_acc=1.000, C2_acc=0.750
Batch 290: C2_loss=0.867, C1_acc=1.000, C2_acc=0.625
Batch 300: C2_loss=0.678, C1_acc=1.000, C2_acc=0.500
Batch 310: C2_loss=0.740, C1_acc=1.000, C2_acc=0.625
Batch 320: C2_loss=0.606, C1_acc=1.000, C2_acc=0.750
Batch 330: C2_loss=1.052, C1_acc=1.000, C2_acc=0.500
Batch 340: C2_loss=1.007, C1_acc=1.000, C2_acc=0.750
Batch 350: C2_loss=1.223, C1_acc=1.000, C2_acc=0.500
Batch 360: C2_loss=0.583, C1_acc=1.000, C2_acc=0.875
Batch 370: C2_loss=0.914, C1_acc=1.000, C2_acc=0.625
Batch 380: C2_loss=0.714, C1_acc=1.000, C2_acc=0.625
Batch 390: C2_loss=1.393, C1_acc=1.000, C2_acc=0.500
Batch 400: C2_loss=0.498, C1_acc=1.000, C2_acc=0.625
Batch 410: C2_loss=0.984, C1_acc=1.000, C2_acc=0.500
Batch 420: C2_loss=0.984, C1_acc=1.000, C2_acc=0.750
Batch 430: C2_loss=0.456, C1_acc=1.000, C2_acc=0.625
Batch 440: C2_loss=0.610, C1_acc=1.000, C2_acc=0.750
Batch 450: C2_loss=1.664, C1_acc=1.000, C2_acc=0.500
Batch 460: C2_loss=0.874, C1_acc=1.000, C2_acc=0.750
Batch 470: C2_loss=0.517, C1_acc=1.000, C2_acc=0.875
Batch 480: C2_loss=0.802, C1_acc=1.000, C2_acc=0.625
Batch 490: C2_loss=1.317, C1_acc=1.000, C2_acc=0.250
Batch 500: C2_loss=1.083, C1_acc=1.000, C2_acc=0.625
Batch 510: C2_loss=0.467, C1_acc=1.000, C2_acc=1.000
Batch 520: C2_loss=0.668, C1_acc=1.000, C2_acc=0.625
Batch 530: C2_loss=0.631, C1_acc=1.000, C2_acc=0.750
Batch 540: C2_loss=1.647, C1_acc=1.000, C2_acc=0.500
Batch 550: C2_loss=1.350, C1_acc=1.000, C2_acc=0.500
Batch 560: C2_loss=0.744, C1_acc=1.000, C2_acc=0.500
Batch 570: C2_loss=0.839, C1_acc=1.000, C2_acc=0.500
Batch 580: C2_loss=0.833, C1_acc=1.000, C2_acc=0.625
Batch 590: C2_loss=0.772, C1_acc=1.000, C2_acc=0.625
Batch 600: C2_loss=0.775, C1_acc=1.000, C2_acc=0.875
Batch 610: C2_loss=0.847, C1_acc=1.000, C2_acc=0.500
Batch 620: C2_loss=1.200, C1_acc=1.000, C2_acc=0.500
Batch 630: C2_loss=0.602, C1_acc=1.000, C2_acc=0.750
Batch 640: C2_loss=0.674, C1_acc=1.000, C2_acc=0.750
Batch 650: C2_loss=0.620, C1_acc=1.000, C2_acc=0.625
Batch 660: C2_loss=0.638, C1_acc=1.000, C2_acc=0.875
Batch 670: C2_loss=0.820, C1_acc=1.000, C2_acc=0.625
Batch 680: C2_loss=0.457, C1_acc=1.000, C2_acc=0.750
Batch 690: C2_loss=0.799, C1_acc=1.000, C2_acc=0.625
Batch 700: C2_loss=0.684, C1_acc=0.875, C2_acc=0.875
Batch 710: C2_loss=1.134, C1_acc=1.000, C2_acc=0.625
Epoch 2 Summary:
  Stage: warmup
\n=== EPOCH 3/3 ===
WARMUP STAGE - Training C2 normally
Batch 0: C2_loss=0.761, C1_acc=1.000, C2_acc=0.625
Batch 10: C2_loss=0.455, C1_acc=1.000, C2_acc=0.875
Batch 20: C2_loss=0.607, C1_acc=1.000, C2_acc=0.875
Batch 30: C2_loss=0.495, C1_acc=1.000, C2_acc=0.750
Batch 40: C2_loss=0.900, C1_acc=1.000, C2_acc=0.625
Batch 50: C2_loss=1.034, C1_acc=1.000, C2_acc=0.625
Batch 60: C2_loss=0.535, C1_acc=1.000, C2_acc=0.625
Batch 70: C2_loss=0.548, C1_acc=1.000, C2_acc=0.625
Batch 80: C2_loss=0.975, C1_acc=1.000, C2_acc=0.375
Batch 90: C2_loss=0.783, C1_acc=1.000, C2_acc=0.750
Batch 100: C2_loss=0.373, C1_acc=1.000, C2_acc=0.875
Batch 110: C2_loss=0.289, C1_acc=1.000, C2_acc=0.875
Batch 120: C2_loss=0.837, C1_acc=1.000, C2_acc=0.625
Batch 130: C2_loss=1.512, C1_acc=1.000, C2_acc=0.375
Batch 140: C2_loss=0.368, C1_acc=1.000, C2_acc=1.000
Batch 150: C2_loss=1.495, C1_acc=1.000, C2_acc=0.500
Batch 160: C2_loss=0.250, C1_acc=1.000, C2_acc=1.000
Batch 170: C2_loss=0.483, C1_acc=1.000, C2_acc=0.875
Batch 180: C2_loss=0.517, C1_acc=1.000, C2_acc=0.750
Batch 190: C2_loss=0.129, C1_acc=1.000, C2_acc=1.000
Batch 200: C2_loss=1.390, C1_acc=1.000, C2_acc=0.500
Batch 210: C2_loss=0.613, C1_acc=1.000, C2_acc=0.750
Batch 220: C2_loss=0.280, C1_acc=1.000, C2_acc=0.875
Batch 230: C2_loss=0.786, C1_acc=1.000, C2_acc=0.500
Batch 240: C2_loss=0.540, C1_acc=1.000, C2_acc=0.750
Batch 250: C2_loss=1.368, C1_acc=1.000, C2_acc=0.500
Batch 260: C2_loss=0.234, C1_acc=1.000, C2_acc=0.875
